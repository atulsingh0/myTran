{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as  plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def logging(name, level):\n",
    "    logger = logging.getLogger(name)\n",
    "    logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n",
    "    logging.root.setLevel(level=level)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inp, out):\n",
    "    logger = logging.getLogger(\"word2vec-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    #logging(\"word2vec-training\", \"logging.INFO\")\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,\n",
    "                    min_count=5, workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    model.init_sims(replace=True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 12:47:41,576 : INFO : collecting all words and their counts\n",
      "2017-09-23 12:47:41,597 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-23 12:47:41,680 : INFO : collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2017-09-23 12:47:41,682 : INFO : Loading a fresh vocabulary\n",
      "2017-09-23 12:47:41,702 : INFO : min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2017-09-23 12:47:41,703 : INFO : min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2017-09-23 12:47:41,722 : INFO : deleting the raw counts dictionary of 11440 items\n",
      "2017-09-23 12:47:41,728 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2017-09-23 12:47:41,731 : INFO : downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2017-09-23 12:47:41,735 : INFO : estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2017-09-23 12:47:41,753 : INFO : resetting layer weights\n",
      "2017-09-23 12:47:41,810 : INFO : training model with 4 workers on 2362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-23 12:47:42,373 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-23 12:47:42,375 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-23 12:47:42,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-23 12:47:42,387 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-23 12:47:42,390 : INFO : training on 525990 raw words (354129 effective words) took 0.6s, 614475 effective words/s\n",
      "2017-09-23 12:47:42,393 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-09-23 12:47:42,427 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2017-09-23 12:47:42,428 : INFO : not storing attribute syn0norm\n",
      "2017-09-23 12:47:42,430 : INFO : not storing attribute cum_table\n",
      "2017-09-23 12:47:42,461 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "train_model(\"data/reddit-small.txt\", \"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 12:58:34,361 : INFO : loading Word2Vec object from word2vec.model\n",
      "2017-09-23 12:58:34,394 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2017-09-23 12:58:34,396 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-09-23 12:58:34,404 : INFO : setting ignored attribute cum_table to None\n",
      "2017-09-23 12:58:34,406 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04299842, -0.00210287, -0.00880035, -0.08522378, -0.08817672,\n",
       "        0.24061784, -0.0367457 , -0.06310907, -0.03914295, -0.02903455,\n",
       "        0.06758396, -0.10153999,  0.09833147, -0.23931736,  0.18460944,\n",
       "        0.00940851,  0.03951518, -0.00881359, -0.16723104, -0.00763027,\n",
       "       -0.09344571, -0.02004526, -0.11682951,  0.00800047, -0.14296281,\n",
       "        0.19845377, -0.10867716,  0.03706482, -0.10137745, -0.1381615 ,\n",
       "        0.06025601,  0.07049808,  0.1427194 , -0.13049455, -0.00603143,\n",
       "        0.02047752, -0.02973599,  0.13603571,  0.05601817,  0.09427408,\n",
       "       -0.00966859, -0.04366736,  0.07928373,  0.02855669, -0.18951257,\n",
       "        0.06192216, -0.06663023, -0.02274433, -0.02263743, -0.07514915,\n",
       "        0.06249481, -0.09202767,  0.01306248,  0.00564152,  0.04090673,\n",
       "        0.08512877, -0.10707233,  0.02166586,  0.01318651,  0.0954906 ,\n",
       "        0.05085073, -0.1216623 ,  0.19160366, -0.0363052 ,  0.13064745,\n",
       "       -0.08109435,  0.19012621,  0.09782867,  0.24658716,  0.02798329,\n",
       "        0.16935232,  0.04689046, -0.15904565, -0.08810442, -0.05795565,\n",
       "        0.11966141, -0.1551784 , -0.04467946, -0.074913  , -0.05742255,\n",
       "        0.03951173, -0.07192744,  0.01787021,  0.07067911,  0.02064017,\n",
       "        0.20142402,  0.05190744,  0.07181177, -0.07697136, -0.07180426,\n",
       "        0.0381519 ,  0.15681325, -0.11872883,  0.04177757, -0.07097446,\n",
       "       -0.10490307,  0.09176835, -0.10658367,  0.00031758,  0.11546472], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.40445745e-02,   4.18875134e-03,  -1.39789591e-02,\n",
       "         -9.19054896e-02,  -9.91533995e-02,   2.47634605e-01,\n",
       "         -4.18907441e-02,  -5.39028496e-02,  -4.56904471e-02,\n",
       "         -2.28010044e-02,   7.95690864e-02,  -9.48444232e-02,\n",
       "          1.12618439e-01,  -2.52312005e-01,   1.97512031e-01,\n",
       "         -4.23302036e-03,   4.94701900e-02,  -2.70220032e-03,\n",
       "         -1.71472237e-01,  -1.47476858e-02,  -9.09075812e-02,\n",
       "         -1.13942986e-02,  -1.17439009e-01,  -2.88570044e-03,\n",
       "         -1.32845864e-01,   2.04405189e-01,  -9.72923562e-02,\n",
       "          4.81134318e-02,  -9.96647626e-02,  -1.33458629e-01,\n",
       "          5.08633479e-02,   6.36581108e-02,   1.36204928e-01,\n",
       "         -1.23721853e-01,  -9.14706662e-03,   3.11423391e-02,\n",
       "         -2.76817698e-02,   1.43542275e-01,   6.36448786e-02,\n",
       "          8.68040845e-02,  -1.78218875e-02,  -3.91467884e-02,\n",
       "          7.20724463e-02,   4.09129784e-02,  -1.82155505e-01,\n",
       "          4.68895510e-02,  -6.26138449e-02,  -2.98548844e-02,\n",
       "         -2.17806473e-02,  -7.70170242e-02,   6.61666766e-02,\n",
       "         -9.22754034e-02,   2.08398309e-02,   1.14335135e-05,\n",
       "          3.03901862e-02,   7.83700570e-02,  -1.10174142e-01,\n",
       "          7.99886417e-03,   1.01504326e-02,   1.02398396e-01,\n",
       "          4.89984117e-02,  -1.22829661e-01,   1.92955792e-01,\n",
       "         -4.73819412e-02,   1.19538166e-01,  -7.78576359e-02,\n",
       "          1.80900291e-01,   9.97279584e-02,   2.38070711e-01,\n",
       "          4.22575250e-02,   1.57629550e-01,   4.35370505e-02,\n",
       "         -1.53501168e-01,  -9.37415212e-02,  -4.96022925e-02,\n",
       "          1.10151529e-01,  -1.66175827e-01,  -4.26406860e-02,\n",
       "         -8.91244635e-02,  -4.53054532e-02,   4.88807969e-02,\n",
       "         -7.03298599e-02,   1.13271354e-02,   6.36925176e-02,\n",
       "          8.60202871e-03,   1.95565343e-01,   4.39611264e-02,\n",
       "          6.28213808e-02,  -7.75425062e-02,  -7.57515058e-02,\n",
       "          4.07084897e-02,   1.60268307e-01,  -1.25404790e-01,\n",
       "          4.85418886e-02,  -8.01980644e-02,  -9.39377695e-02,\n",
       "          9.98580530e-02,  -1.12717099e-01,   2.63526710e-03,\n",
       "          1.03275701e-01]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[['queen']]  # model will throw the error if the word is not existing in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
