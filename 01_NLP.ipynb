{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as  plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim-2.3.0-py3.5-win-amd64.egg\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import multiprocessing\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def logging(name, level):\n",
    "    logger = logging.getLogger(name)\n",
    "    logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n",
    "    logging.root.setLevel(level=level)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(inp, out):\n",
    "    logger = logging.getLogger(\"word2vec-training\")\n",
    "    logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\")\n",
    "    logging.root.setLevel(level=logging.INFO)\n",
    "    #logging(\"word2vec-training\", \"logging.INFO\")\n",
    "    \n",
    "    model = Word2Vec(LineSentence(inp), size=100, window=5,\n",
    "                    min_count=5, workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    model.init_sims(replace=True)\n",
    "    model.save(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 15:57:05,594 : INFO : collecting all words and their counts\n",
      "2017-09-23 15:57:05,615 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-09-23 15:57:05,774 : INFO : collected 11440 word types from a corpus of 105198 raw words and 5000 sentences\n",
      "2017-09-23 15:57:05,775 : INFO : Loading a fresh vocabulary\n",
      "2017-09-23 15:57:05,794 : INFO : min_count=5 retains 2362 unique words (20% of original 11440, drops 9078)\n",
      "2017-09-23 15:57:05,797 : INFO : min_count=5 leaves 90968 word corpus (86% of original 105198, drops 14230)\n",
      "2017-09-23 15:57:05,814 : INFO : deleting the raw counts dictionary of 11440 items\n",
      "2017-09-23 15:57:05,819 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2017-09-23 15:57:05,822 : INFO : downsampling leaves estimated 70796 word corpus (77.8% of prior 90968)\n",
      "2017-09-23 15:57:05,824 : INFO : estimated required memory for 2362 words and 100 dimensions: 3070600 bytes\n",
      "2017-09-23 15:57:05,842 : INFO : resetting layer weights\n",
      "2017-09-23 15:57:05,901 : INFO : training model with 4 workers on 2362 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-09-23 15:57:06,480 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-09-23 15:57:06,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-09-23 15:57:06,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-09-23 15:57:06,494 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-09-23 15:57:06,496 : INFO : training on 525990 raw words (354129 effective words) took 0.6s, 600431 effective words/s\n",
      "2017-09-23 15:57:06,498 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-09-23 15:57:06,539 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2017-09-23 15:57:06,540 : INFO : not storing attribute syn0norm\n",
      "2017-09-23 15:57:06,542 : INFO : not storing attribute cum_table\n",
      "2017-09-23 15:57:06,570 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "train_model(\"data/reddit-small.txt\", \"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 15:57:06,581 : INFO : loading Word2Vec object from word2vec.model\n",
      "2017-09-23 15:57:06,665 : INFO : loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "2017-09-23 15:57:06,666 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-09-23 15:57:06,668 : INFO : setting ignored attribute cum_table to None\n",
      "2017-09-23 15:57:06,671 : INFO : loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -4.71556745e-03,   4.69302833e-02,   1.38287038e-01,\n",
       "        -3.66218458e-03,   7.15303123e-02,   5.62157221e-02,\n",
       "        -8.60737562e-02,   6.27359748e-02,  -7.36661442e-03,\n",
       "        -1.58841148e-01,   4.23208512e-02,   1.38949985e-02,\n",
       "         1.83057617e-02,   2.88402699e-02,   6.04900308e-02,\n",
       "         1.83993295e-01,   1.73676297e-01,  -2.65602320e-02,\n",
       "        -1.46508232e-01,  -1.07726209e-01,   9.79714096e-02,\n",
       "         5.27226329e-02,   2.91530229e-03,   6.25990033e-02,\n",
       "        -6.74230307e-02,  -1.66072801e-01,  -4.92809936e-02,\n",
       "        -7.08616152e-03,  -1.45713642e-01,   1.00677744e-01,\n",
       "        -8.28253180e-02,  -8.98007676e-02,  -2.96194553e-02,\n",
       "        -8.60425120e-04,   7.13192075e-02,  -4.98724245e-02,\n",
       "        -6.44445866e-02,   6.31915778e-02,   2.86446977e-03,\n",
       "         1.22054495e-01,   4.06177305e-02,  -7.61726648e-02,\n",
       "         8.48585963e-02,   9.49178115e-02,  -1.41638592e-01,\n",
       "        -1.10547736e-01,   1.49406374e-01,   6.58446923e-02,\n",
       "         8.06896761e-02,  -2.44487654e-02,  -2.05161907e-02,\n",
       "        -1.43394917e-01,  -3.48811485e-02,   7.82556832e-02,\n",
       "         1.77802458e-01,  -4.10586409e-02,   7.56347581e-05,\n",
       "         5.52600957e-02,   1.46786079e-01,  -1.19348504e-01,\n",
       "         3.08782957e-03,  -2.67951712e-02,   2.66319126e-01,\n",
       "        -7.33796181e-03,   1.56905249e-01,  -1.21118557e-02,\n",
       "        -3.18440735e-01,  -4.01907079e-02,  -1.26294404e-01,\n",
       "        -8.01083446e-02,  -6.53715506e-02,   5.36145493e-02,\n",
       "        -3.30226719e-02,  -1.96977898e-01,  -7.75367022e-02,\n",
       "         6.55579343e-02,  -1.46089375e-01,   6.65180981e-02,\n",
       "         8.83594304e-02,   1.18450075e-03,   5.62574081e-02,\n",
       "        -2.20513865e-02,   1.23762516e-02,   6.85049547e-03,\n",
       "         2.45566726e-01,   1.20876677e-01,  -1.07939355e-01,\n",
       "         9.80644152e-02,   4.41089571e-02,  -2.52181315e-03,\n",
       "        -1.00417241e-01,  -3.95790562e-02,  -9.89918187e-02,\n",
       "         1.71401724e-01,   9.68559533e-02,  -8.12281817e-02,\n",
       "         7.30810761e-02,   5.07120974e-02,  -5.73506467e-02,\n",
       "         1.32164851e-01], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['money'] # model will throw the error if the word is not existing in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-23 15:57:06,716 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('which', 0.9999130964279175),\n",
       " ('long', 0.9998975396156311),\n",
       " ('better', 0.9998939037322998),\n",
       " ('point', 0.9998928904533386),\n",
       " ('though', 0.9998907446861267),\n",
       " ('around', 0.9998847246170044),\n",
       " ('most', 0.9998843669891357),\n",
       " ('come', 0.9998841285705566),\n",
       " ('take', 0.9998818039894104),\n",
       " ('god', 0.9998811483383179)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99973765497596645"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('money', 'amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(inp1, inp2):\n",
    "    return np.dot(inp1, inp2)/(np.linalg.norm(inp1)*np.linalg.norm(inp2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914601339836675"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1,2,3], [1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_similarity(text1, text2):\n",
    "    text1 = text1.lower().split()\n",
    "    text2 = text2.lower().split()\n",
    "    \n",
    "    # generating word vectors\n",
    "    vec1 = np.array([model[word] for word in text1])\n",
    "    vec2 = np.array([model[word] for word in text2])\n",
    "    \n",
    "    avg_vec1 = np.mean(vec1, axis=0)\n",
    "    avg_vec2 = np.mean(vec2, axis=0)\n",
    "    \n",
    "    return cosine_similarity(avg_vec1, avg_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"the king money\", \"happy about long story\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99981719"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_similarity(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Ramanujan was the great mathematician.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramanujan PROPN NNP PERSON 0.0\n",
      "was VERB VBD  0.199599749025\n",
      "the DET DT  0.190436797865\n",
      "great ADJ JJ  0.228298441797\n",
      "mathematician NOUN NN  0.382748813508\n",
      ". PUNCT .  0.209577768661\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_, token.ent_type_,  token.similarity(nlp(\"math\")))  \n",
    "    # token.vector - this will display the word vector, it will be 0 if the word is not available in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ramanujan,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "builtin_function_or_method"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.print_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ramanujan, the great mathematician]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ramanujan was the great mathematician.]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
